{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feedforward_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaoYamado/notebooks/blob/master/Feedforward_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irQs0lPh-r1k",
        "colab_type": "text"
      },
      "source": [
        "# Feedforward Neural Network with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEbiO5P_-3C1",
        "colab_type": "text"
      },
      "source": [
        "## About Feedforward Neural Network\n",
        "### Logistic Regression Transition to Neural Network\n",
        "#### Logistic Regression Review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKXqrGv-i4y",
        "colab_type": "code",
        "outputId": "80c6491d-d57a-4c18-8500-ec7441d93acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Define logistic regression model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define our model class\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LogisticRegressionModel, self).__init__()\n",
        "    self.linear = nn.Linear(input_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.linear(x)\n",
        "    return out\n",
        "\n",
        "# instantiate the logistic regression model\n",
        "input_dim = 28*28\n",
        "output_dim = 10\n",
        "\n",
        "model = LogisticRegressionModel(input_dim, output_dim)\n",
        "print(model)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegressionModel(\n",
            "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WVvhv8VL_LW",
        "colab_type": "text"
      },
      "source": [
        "## Build Feedforward Neural Network with PyTorch\n",
        "\n",
        "#### Model A: 1 Hidden layer Feedforward Neural Network (Sigmoid activation)\n",
        "![alt text](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/nn1.png)\n",
        "\n",
        "#### Steps:\n",
        "*   Step 1: Load Dataset\n",
        "*   Step 2: Make Dataset Iterable\n",
        "*   Step 3: Create Model Class\n",
        "*   Step 4: Instantiate Model Class\n",
        "*   Step 5: Instantiate Loss Class\n",
        "*   Step 6: Instantiate Optimizer Class\n",
        "*   Step 7: Train Model\n",
        "\n",
        "## Step 1: Loading MNIST Train Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP82KC9vATgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data',\n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "                          train=False,\n",
        "                          transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eqtap7FOSwK",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Make Dataset Iterable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDI0XyCoOORu",
        "colab_type": "code",
        "outputId": "10798dc9-3c5f-496d-8d2d-951f1f1c8a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch sizes and iterations\n",
        "\n",
        "60000/100"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux-jD6WkPOw7",
        "colab_type": "code",
        "outputId": "ec0f36b2-21f5-49b4-b0fc-7a257394e57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " # Epochs\n",
        " 600.0 * 5"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VRj0cokPahy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Brinding batch size, iterations and epochs together\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKxhX6CkwSH_",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Create Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhPVPtuAQlew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(FeedforwardNeuralNetModel, self).__init__()\n",
        "    # linear function\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "    # Non-linearity\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Linear function (readout)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # linear function # LINEAR\n",
        "    out = self.fc1(x)\n",
        "\n",
        "    # Non-linearity # NON LINEAR\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    # Linear function (readout) # LINEAR\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Q_2sgOyDdo",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Instantiate Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwzNA7HPx70F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0zuE-BZyyx1",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Instantiate Loss Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfaxk61ayxpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE0k6IolzF7n",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Instantiate Optimizer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOMxMfz2zEv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1p64b_B0CSv",
        "colab_type": "code",
        "outputId": "b7617e14-e3bd-4141-c893-e01d9fb5b395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(model.parameters())\n",
        "print(len(list(model.parameters())))\n",
        "# FC 1\n",
        "print(list(model.parameters())[0].size())\n",
        "# FC 1 Bieas Parameters\n",
        "print(list(model.parameters())[1].size())\n",
        "# FC 2 Parameters\n",
        "print(list(model.parameters())[2].size())\n",
        "# FC 2 Bias Parameters\n",
        "print(list(model.parameters())[3].size())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Module.parameters at 0x7f8bc9442888>\n",
            "4\n",
            "torch.Size([100, 784])\n",
            "torch.Size([100])\n",
            "torch.Size([10, 100])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5UepHjn2MC_",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/nn1_params3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BOkShs92S_1",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Train Model\n",
        "#### Process:\n",
        "*   a. Convert inputs to tensors with gradients accumulation capabilities\n",
        "*   b. Clear gradients buffers\n",
        "*   c. Get output given inputs\n",
        "*   d. Get loss\n",
        "*   e. Get gradients w.r.t. parameters\n",
        "*   f. Update parameters using gradients\n",
        "    *   parameters = parameters - learning_rate * parameters_gradients\n",
        "*   g. REPEAT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1b3gZvy1kdk",
        "colab_type": "code",
        "outputId": "02212c47-290d-495b-fd5b-5da02aa9f5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# training process\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # load images with gradient accumulation capabilities\n",
        "    images = images.view(-1, 28*28).requires_grad_()\n",
        "    # clear gradients w.r.t. parameters\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass to get output/logits\n",
        "    outputs = model(images)\n",
        "    # Calculate Loss: softmax ---> cross entropy loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    # updating parameters\n",
        "    optimizer.step()\n",
        "    iter += 1\n",
        "\n",
        "    if iter % 500 == 0:\n",
        "      # Calculate Accuracy\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      # iterate through test dataset\n",
        "      for images, labels in test_loader:\n",
        "        # load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        # forward pass only to get logits/ouput\n",
        "        outputs = model(images)\n",
        "        # get predictions from maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #  total number of labels\n",
        "        total += labels.size(0)\n",
        "        # Total correct predictions\n",
        "        correct += (predicted == labels).sum()\n",
        "      accuracy = 100 * correct/total\n",
        "\n",
        "      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6887582540512085. Accuracy: 86\n",
            "Iteration: 1000. Loss: 0.46380242705345154. Accuracy: 89\n",
            "Iteration: 1500. Loss: 0.43305614590644836. Accuracy: 90\n",
            "Iteration: 2000. Loss: 0.2959619462490082. Accuracy: 91\n",
            "Iteration: 2500. Loss: 0.3037315607070923. Accuracy: 91\n",
            "Iteration: 3000. Loss: 0.3642258048057556. Accuracy: 92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJWqZ5M8cGa",
        "colab_type": "text"
      },
      "source": [
        "# Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennWugXZj8Pv",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/nn1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BapEBxH6kBSL",
        "colab_type": "text"
      },
      "source": [
        "#### Steps:\n",
        "\n",
        "\n",
        "*   Step 1: Load Dataset\n",
        "*   Step 2: Make Dataset iterable\n",
        "*   Step 3: Create Model Class\n",
        "*   Step 4: Instantiate Model Class\n",
        "*   Step 5: Instantiate Loss Class\n",
        "*   Step 6: Insantiate Optimizer Class\n",
        "*   Step 7: Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbOrn2cj7u8j",
        "colab_type": "code",
        "outputId": "0a6edf80-a7ed-4ba3-d746-4809d9b2056e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# 1-layer with Tanh activation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "\"\"\"\n",
        "STEP 1: LOADING DATASET\n",
        "\"\"\"\n",
        "train_dataset = dsets.MNIST(root='./data',\n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "\"\"\"\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "\"\"\"\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(FeedforwardNeuralNetModel, self).__init__()\n",
        "    # Linear Function\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    # Non-lenearity\n",
        "    self.tanh = nn.Tanh()\n",
        "    # Linear function (readout)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # Linear funstion\n",
        "    out = self.fc1(x)\n",
        "    # Non-linearity\n",
        "    out = self.tanh(out)\n",
        "    # Linear function (readout)\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "\"\"\"\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "\"\"\"\n",
        "imput_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "\"\"\"\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "\"\"\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\"\"\"\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "\"\"\"\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\"\"\"\n",
        "STEP 7: TRAIN THE MODEL\n",
        "\"\"\"\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # load images with gradient accumulation capabilities\n",
        "    images = images.view(-1, 28*28).requires_grad_()\n",
        "    #Clear gradient w.r.t. parameters\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass to get output.logits\n",
        "    outputs = model(images)\n",
        "    # calculate Los: softmax ---> cross entropy loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    # Updating paramers\n",
        "    optimizer.step()\n",
        "\n",
        "    iter += 1\n",
        "    if iter % 500 == 0:\n",
        "      # Calculate Accuracy\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      # iterate through test dataset\n",
        "      for images, labels in test_loader:\n",
        "        # load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        # forward pass only to get logits/output\n",
        "        outputs = model(images)\n",
        "        # get prediction from maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # total number of labels\n",
        "        total += labels.size(0)\n",
        "        # total correct predictions\n",
        "        correct += (predicted == labels).sum()\n",
        "      accuracy = 100 * correct / total\n",
        "      # print loss \n",
        "      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.49559611082077026. Accuracy: 91\n",
            "Iteration: 1000. Loss: 0.3960130214691162. Accuracy: 92\n",
            "Iteration: 1500. Loss: 0.21708333492279053. Accuracy: 93\n",
            "Iteration: 2000. Loss: 0.1590883880853653. Accuracy: 93\n",
            "Iteration: 2500. Loss: 0.1721712350845337. Accuracy: 94\n",
            "Iteration: 3000. Loss: 0.20714856684207916. Accuracy: 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wmL3L3iK4ev",
        "colab_type": "text"
      },
      "source": [
        "# Model C: 1 Hidden Layer Feedforward Neural Network (ReLU activation)\n",
        "![alt text](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/nn1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMK4hN4_Jt8c",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "ef0029cb-39b4-45fe-9730-8e10f90c3455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#@title Model C: Hidden layer(Relu activation)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "\"\"\"\n",
        "STEP 1: LOADING DATASET\n",
        "\"\"\"\n",
        "\n",
        "train_datset = dsets.MNIST(root='./data',\n",
        "                           train=True,\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "\"\"\"\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "\"\"\"\n",
        "\n",
        "batch_size =100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters /(len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\"\"\"\n",
        "STEP 3: Create Model Class\n",
        "\"\"\"\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(FeedforwardNeuralNetModel, self).__init__()\n",
        "    # Linear function\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    # Non-Linearity\n",
        "    self.relu = nn.ReLU()\n",
        "    # Linear function (readout)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "     # linear function\n",
        "     out = self.fc1(x)\n",
        "     # Non-linearity\n",
        "     out = self.relu(out)\n",
        "     # Linear function (readout)\n",
        "     out = self.fc2(out)\n",
        "     return out\n",
        "    \n",
        "\"\"\"\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "\"\"\"\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\"\"\"\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "\"\"\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\"\"\"\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "\"\"\"\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\"\"\"\n",
        "STEP 7: TRAIN THE MODEL\n",
        "\"\"\"\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # load images with gradient accumulation capabilities\n",
        "    images = images.view(-1, 28*28).requires_grad_()\n",
        "    # Clear gradients w.r.t. parameters\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass to get output/logits\n",
        "    outputs = model(images)\n",
        "    # Calculate Loss: softmax--->cross entropy loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    # updating parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    iter += 1\n",
        "    if iter % 500 == 0:\n",
        "      # Calculate Accuracy\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      # Iterate through test dataset\n",
        "      for images, labels in test_loader:\n",
        "        # Load images with gradients accumulation capabilities \n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        # forward pass only to get logits/output\n",
        "        outputs = model(images)\n",
        "        # get predictions form the maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # total number of labels \n",
        "        total += labels.size(0)\n",
        "        # total correct predictions\n",
        "        correct += (predicted == labels).sum()\n",
        "      accuracy = 100 * correct / total\n",
        "      # print loss\n",
        "      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.42539066076278687. Accuracy: 91\n",
            "Iteration: 1000. Loss: 0.25788912177085876. Accuracy: 92\n",
            "Iteration: 1500. Loss: 0.15192098915576935. Accuracy: 93\n",
            "Iteration: 2000. Loss: 0.3578575849533081. Accuracy: 94\n",
            "Iteration: 2500. Loss: 0.12928007543087006. Accuracy: 95\n",
            "Iteration: 3000. Loss: 0.17368324100971222. Accuracy: 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktLS0UEcotDI",
        "colab_type": "text"
      },
      "source": [
        "# Model D: 2 Hidden Layer Feedforward Neural Network(ReLU activation)\n",
        "![alt text](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/nn2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkSOC0iwwD42",
        "colab_type": "code",
        "outputId": "d02cfe6d-5463-451d-d513-5336c0bba49b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function 1: 784 --> 100\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity 1\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Linear function 2: 100 --> 100\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # Non-linearity 2\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Linear function 3 (readout): 100 --> 10\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity 1\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Linear function 3 (readout)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        labels = labels\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.2943076193332672. Accuracy: 91\n",
            "Iteration: 1000. Loss: 0.19378866255283356. Accuracy: 93\n",
            "Iteration: 1500. Loss: 0.14716047048568726. Accuracy: 95\n",
            "Iteration: 2000. Loss: 0.1172453910112381. Accuracy: 95\n",
            "Iteration: 2500. Loss: 0.03787495568394661. Accuracy: 96\n",
            "Iteration: 3000. Loss: 0.0711575448513031. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKuPyL1fxQCv",
        "colab_type": "text"
      },
      "source": [
        "# Model E: 3 Hidden Layer Feedforward Neural Network(ReLU activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "andzQgUxw21A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "260e146e-4be8-4138-e667-4c7f77837312"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "\"\"\"\n",
        "STEP 1: LOADING DATASET\n",
        "\"\"\"\n",
        "train_dataset = dsets.MNIST(root='./data',\n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "                           train=False,\n",
        "                           transform = transforms.ToTensor())\n",
        "\n",
        "\"\"\"\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 100\n",
        "n_iter = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\"\"\"\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "\"\"\"\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(FeedforwardNeuralNetModel, self).__init__()\n",
        "    # Linear function 1: 784 ---> 100\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    # Non-linearity\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    # linear function 2: 100 ---> 100\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    # Non-linearity\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    # Linear function 3: 100 ---> 100\n",
        "    self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    # Non-linearity\n",
        "    self.relu3 = nn.ReLU()\n",
        "\n",
        "    # Linear function 4 (readout): 100 ---> 10\n",
        "    self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # linear function 1\n",
        "    out = self.fc1(x)\n",
        "    # Non-linearity 1\n",
        "    out = self.relu1(out)\n",
        "\n",
        "    # Linear function 2\n",
        "    out = self.fc2(out)\n",
        "    # Non-linearity 2\n",
        "    out = self.relu2(out)\n",
        "\n",
        "    # Linear function 3\n",
        "    out = self.fc3(out)\n",
        "    # Non-linearity 3 \n",
        "    out = self.relu3(out)\n",
        "\n",
        "    # Linear function 4 (readout)\n",
        "    out = self.fc4(out)\n",
        "    return out\n",
        "  \n",
        "\"\"\"\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "\"\"\"\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "\"\"\"\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "\"\"\"\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\"\"\"\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "\"\"\"\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\"\"\"\n",
        "STEP 7: TRAIN THE MODEL\n",
        "\"\"\"\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # load images with gradient accumulation capabilities\n",
        "    images = images.view(-1, 28*28).requires_grad_()\n",
        "    # clear gradients w.r.t. parameters\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass to get output/logits\n",
        "    outputs = model(images)\n",
        "    # Calculate Loss: softmax ---> cross entropy loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    # Updating parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    iter += 1\n",
        "    if iter % 500 == 0:\n",
        "      # Calculate Accuracy\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      # Iterate through test dataset\n",
        "      for images, labels in test_loader:\n",
        "        # load images with gradients accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        # Forward pass only to get logits/output\n",
        "        outputs = model(images)\n",
        "        # get predictions from maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # total number of label\n",
        "        total += labels.size(0)\n",
        "        # total correct predictions\n",
        "        correct += (predicted == labels).sum()\n",
        "      accuracy = 100 * correct / total\n",
        "      print('Iteration : {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration : 500. Loss: 0.2598670721054077. Accuracy: 90\n",
            "Iteration : 1000. Loss: 0.11560507863759995. Accuracy: 94\n",
            "Iteration : 1500. Loss: 0.13716332614421844. Accuracy: 95\n",
            "Iteration : 2000. Loss: 0.15583257377147675. Accuracy: 96\n",
            "Iteration : 2500. Loss: 0.0744999572634697. Accuracy: 96\n",
            "Iteration : 3000. Loss: 0.04102780669927597. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zyyzz2GUvtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}