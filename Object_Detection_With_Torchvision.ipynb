{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_Detection_With_Torchvision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaoYamado/notebooks/blob/master/Object_Detection_With_Torchvision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC11slUT9yw5",
        "colab_type": "text"
      },
      "source": [
        "# Torchvision Object Detection\n",
        "\n",
        "> Examples given from official documentation PyTorch\n",
        "\n",
        "\n",
        "![alt text](https://www.cis.upenn.edu/~jshi/ped_html/images/PennPed00015_1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD2hz6PT9nZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "  def __init__(self, root, transforms):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them,\n",
        "    # ensure that they are aligned\n",
        "    self.imgs = list(sorted(os.listdir(os.path.join(root, 'https://drive.google.com/open?id=1rxfPo7TVrTMheRK1R6scSxD2GtZ0dGxc/')))) # PNGImages\n",
        "    self.masks = list(sorted(os.listdir(os.path.join(root, 'https://drive.google.com/open?id=1gjzGqtZ0m_3VhKREp2U582Pbm7l62fWy')))) # PedMasks\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load images ad masks\n",
        "    img_path = os.path.join(self.root, 'PNGImages', self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, 'PedMasks', self.masks[idx])\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Note that we haven't converted the mask to RGB\n",
        "    # because each color corresponds to a different instance\n",
        "    # with 0 being background\n",
        "    mask = Image.open(mask_path)\n",
        "    # convert the PIL image into a numpy array\n",
        "    mask = np.array(mask)\n",
        "    # instances are encoded as different colors\n",
        "    obj_ids = np.unique(mask)\n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # split the color-encoded mask into a set\n",
        "    # of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    # get bounding box coordinates for each mask\n",
        "    num_obj = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "      pos = np.where(masks[i])\n",
        "      xmin = np.min(pos[1])\n",
        "      xmax = np.max(pos[1])\n",
        "      ymin = np.min(pos[0])\n",
        "      ymax = np.max(pos[0])\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    # converting everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    masks = torch.as_tensor(masks, dtype=torch.utint8)\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_obj,), dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target['boxes'] = boxes\n",
        "    target['labels'] = labels\n",
        "    target['masks'] = masks\n",
        "    target['image_id'] = image_id\n",
        "    target['area'] = area\n",
        "    target['iscrowd'] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "    \n",
        "    return img, target\n",
        "\n",
        "def __len__(self):\n",
        "  return len(self.imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mTvb2Ykq6dY",
        "colab_type": "text"
      },
      "source": [
        "# Defining model\n",
        "\n",
        "In this notebooks, we will using Mask R_CNN(paper in repository), which is based  on top os Fster R_CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential object in the image\n",
        " \n",
        "![alt text](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png) Mask R-CNN adds an extra branch into Fater R-CNN, which also predicts segmentation masks for each instance.\n",
        "![alt text](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image04.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEQRaiphswX6",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Finetuning from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPMQpJqCbsuc",
        "colab_type": "code",
        "outputId": "f9beb2c0-db26-41bd-de08-d47bf3c8d63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2 # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:04<00:00, 33.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hi3Lz9hRAvm",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Modifying the model to add a different backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNqDuWpbxKf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone/ For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# rations. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect rations\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                          aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the sie of the crop after rescaling\n",
        "# if your backbone return a Tensor, featmap_names is expectd to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGTE3jJgbq3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}